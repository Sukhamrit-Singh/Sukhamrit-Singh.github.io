<!DOCTYPE html>
<html>
<head>
    <title>Project 4 - CS180</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            line-height: 1.8;
            color: #e0e0e0;
            background: #0a0a0a;
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 60px 80px;
        }

        h1 {
            font-size: 2.5em;
            font-weight: 300;
            letter-spacing: -1px;
            color: #ffffff;
            margin-bottom: 8px;
        }

        .subtitle {
            font-size: 1em;
            color: #808080;
            margin-bottom: 60px;
            font-weight: 400;
        }

        .author {
            color: #606060;
            margin: 10px 0;
            font-size: 1em;
        }

        h2 {
            color: #ffffff;
            font-size: 1.8em;
            font-weight: 300;
            margin: 80px 0 40px;
            letter-spacing: -0.5px;
        }

        h3 {
            color: #c0c0c0;
            font-size: 1.3em;
            font-weight: 300;
            margin: 40px 0 20px;
        }

        p {
            color: #a0a0a0;
            margin-bottom: 20px;
            line-height: 1.8;
        }

        ul, ol {
            color: #a0a0a0;
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 8px;
        }

        code {
            background: #1a1a1a;
            color: #80ff80;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: inherit;
        }

        pre {
            background: #1a1a1a;
            color: #80ff80;
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: inherit;
        }

        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .image-wrapper {
            margin-bottom: 20px;
        }

        .image-container {
            position: relative;
            overflow: hidden;
            background: #1a1a1a;
            border-radius: 4px;
            transition: transform 0.2s ease;
        }

        .image-container:hover {
            transform: scale(1.02);
        }

        .image-container img {
            width: 100%;
            height: auto;
            display: block;
            opacity: 0.9;
            transition: opacity 0.2s ease;
        }

        .image-container:hover img {
            opacity: 1;
        }

        .image-caption {
            color: #ffffff;
            font-size: 0.95em;
            margin-top: 10px;
            text-align: center;
        }

        .offset-info {
            font-size: 0.85em;
            color: #606060;
            margin-top: 4px;
            text-align: center;
        }

        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 40px 0;
        }

        .comparison-item {
            text-align: center;
        }

        .comparison-item img {
            width: 100%;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .comparison-label {
            color: #808080;
            font-size: 0.9em;
        }

        .algorithm-box {
            background: #1a1a1a;
            border-left: 3px solid #404040;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .highlight {
            background: #1a1a1a;
            border-left: 3px solid #ffff00;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin: 40px 0;
        }

        .results-table th {
            background: #1a1a1a;
            color: #ffffff;
            padding: 12px;
            text-align: left;
            font-weight: 400;
        }

        .results-table td {
            padding: 10px;
            border-bottom: 1px solid #2a2a2a;
            color: #a0a0a0;
        }

        .results-table tr:hover {
            background: #151515;
        }

        .back-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: #1a1a1a;
            border: 1px solid #404040;
            color: #808080;
            padding: 8px 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 8px;
            text-decoration: none;
            transition: all 0.2s ease;
            z-index: 1000;
            font-size: 14px;
        }
        
        .back-button:hover {
            background: #2a2a2a;
            color: #ffffff;
            border-color: #606060;
        }

        .triple-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 40px 0;
        }

        .quad-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            grid-template-rows: 1fr 1fr;
            gap: 20px;
            margin: 40px 0;
        }

        .training-progression {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }

        .video-container {
            display: flex;
            justify-content: center;
            margin: 40px 0;
        }

        .video-container video {
            max-width: 100%;
            border-radius: 4px;
        }

        .psnr-info {
            background: #1a1a1a;
            border-left: 3px solid #40ff40;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .large-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 40px 0;
            max-width: 80%;
            margin-left: auto;
            margin-right: auto;
        }

        .frustum-container {
            background: #f0f0f0;
            border-radius: 4px;
            padding: 20px;
            aspect-ratio: 1920 / 934;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .frustum-container img {
            width: 100%;
            height: 100%;
            object-fit: contain;
        }

        .ray-frustum-container img {
            width: 120%;
            height: 120%;
            object-fit: contain;
            transform: translateY(8%);
        }

        @media (max-width: 768px) {
            .container {
                padding: 40px;
            }
            
            .image-grid {
                grid-template-columns: 1fr;
            }
            
            .comparison {
                grid-template-columns: 1fr;
            }

            .triple-comparison {
                grid-template-columns: 1fr;
            }

            .quad-grid {
                grid-template-columns: 1fr;
                grid-template-rows: auto;
            }
        }

        .equation {
            background: transparent;
            color: #80ff80;
            padding: 12px 6px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 6px 0;
            font-family: inherit;
            white-space: nowrap;
        }

        .matrix-block {
            display: flex;
            gap: 20px;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            margin: 18px 0;
        }
    </style>
    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>
<body>
    <a href="./index.html" class="back-button" title="Back to portfolio">
        <span style="font-size: 14px;">../</span>
    </a>
    
    <div class="container">
        <div style="margin-bottom: 60px;">
            <h1 style="margin-bottom: 20px;">proj4/</h1>
            <div style="margin-left: 40px;">
                <div style="margin: 8px 0;">
                    <a href="#overview" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">overview/</span>
                        <span style="color: #606060; margin-left: 8px;">Neural Radiance Fields</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part0" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part0/</span>
                        <span style="color: #606060; margin-left: 8px;">Camera Calibration and 3D Scanning</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part1" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part1/</span>
                        <span style="color: #606060; margin-left: 8px;">2D Neural Field</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part2" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part2/</span>
                        <span style="color: #606060; margin-left: 8px;">3D Neural Radiance Field</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part2_6" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part2.6/</span>
                        <span style="color: #606060; margin-left: 8px;">Own Data NeRF</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#summary" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">└── </span>
                        <span style="color: #ffffff;">summary/</span>
                        <span style="color: #606060; margin-left: 8px;">Project Summary and Results</span>
                    </a>
                </div>
            </div>
        </div>

        <h1 style="font-size: 2.5em; margin-bottom: 10px;">Neural Radiance Fields!</h1>
        <div style="color: #808080; margin-bottom: 40px; font-size: 1em;">CS180 Project 4: Neural Radiance Fields and Volume Rendering</div>

        <h2 id="overview">Overview</h2>
        <p>
            This project explores Neural Radiance Fields (NeRF), a cutting-edge technique for 3D scene representation 
            and novel view synthesis. We implement neural networks that learn implicit representations of 3D scenes 
            from multi-view images, enabling photorealistic novel view synthesis. The project spans from 2D neural 
            fields to full 3D NeRF implementation, including camera calibration, volume rendering, and training on 
            both synthetic and real-world data.
        </p>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #ff4040;">
            <h2 id="part0" style="margin: 0; color: #ff4040;">Part 0: Camera Calibration and 3D Scanning</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">ArUco-based camera calibration and pose estimation</div>
        </div>

        <p>
            For this section, I calibrated my camera using ArUco tags and captured multi-view images of a chosen object. 
            The calibration process involved capturing 30-50 images of ArUco calibration tags, extracting corner coordinates, 
            and using OpenCV's camera calibration to compute intrinsic parameters and distortion coefficients.
        </p>

        <p>
            After calibration, I captured images of my chosen object (a small figurine) alongside a single ArUco tag, 
            then used the calibrated camera parameters to estimate camera poses via solvePnP. The final step involved 
            undistorting images and packaging everything into a dataset format compatible with NeRF training.
        </p>

        <h3>Camera Frustum Visualizations</h3>
        <p>Below are screenshots of the camera frustums visualization in Viser, showing the estimated camera poses:</p>

        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container frustum-container">
                    <img src="proj4/part0_content/vaja_frustum1.png" alt="Camera Frustums 1">
                </div>
                <div class="comparison-label">Camera Frustums Visualization - View 1</div>
            </div>
            <div class="comparison-item">
                <div class="image-container frustum-container">
                    <img src="proj4/part0_content/vaja_frustum2.png" alt="Camera Frustums 2">
                </div>
                <div class="comparison-label">Camera Frustums Visualization - View 2</div>
            </div>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #40ff40;">
            <h2 id="part1" style="margin: 0; color: #40ff40;">Part 1: 2D Neural Field</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">Fitting neural networks to represent 2D images</div>
        </div>

        <p>
            Before implementing 3D NeRF, I started with a simpler 2D version to understand the fundamentals. 
            I created a Multilayer Perceptron (MLP) with Sinusoidal Positional Encoding that takes 2D pixel 
            coordinates as input and outputs RGB color values. The network was trained to fit entire images 
            by optimizing pixel-wise color predictions.
        </p>

        <h3>Model Architecture</h3>
        <div class="algorithm-box">
            <h4>Network Details:</h4>
            <ul>
                <li><strong>Input:</strong> 2D coordinates (x, y) with sinusoidal positional encoding</li>
                <li><strong>Architecture:</strong> 4-layer MLP with ReLU activations</li>
                <li><strong>Width:</strong> 256 hidden units per layer</li>
                <li><strong>Output:</strong> RGB values with sigmoid activation</li>
                <li><strong>Positional Encoding:</strong> L=10 frequency levels</li>
                <li><strong>Learning Rate:</strong> 1e-2 with Adam optimizer</li>
                <li><strong>Training:</strong> 3000 iterations with batch size of 10k pixels</li>
            </ul>
        </div>

        <h3>Training Progression</h3>
        <p>Below shows the training progression for both the provided fox image and my own Messi image:</p>

        <h4>Fox Image Training</h4>
        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/epoch_0.png" alt="Fox Epoch 0">
                </div>
                <div class="comparison-label">Epoch 0</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/epoch_100.png" alt="Fox Epoch 100">
                </div>
                <div class="comparison-label">Epoch 100</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/epoch_400.png" alt="Fox Epoch 400">
                </div>
                <div class="comparison-label">Epoch 400</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/epoch_1600.png" alt="Fox Epoch 1600">
                </div>
                <div class="comparison-label">Epoch 1600</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/epoch_2999.png" alt="Fox Epoch 2999">
                </div>
                <div class="comparison-label">Final (Epoch 2999)</div>
            </div>
        </div>

        <h4>Messi Image Training</h4>
        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/epoch_0.png" alt="Messi Epoch 0">
                </div>
                <div class="comparison-label">Epoch 0</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/epoch_100.png" alt="Messi Epoch 100">
                </div>
                <div class="comparison-label">Epoch 100</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/epoch_400.png" alt="Messi Epoch 400">
                </div>
                <div class="comparison-label">Epoch 400</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/epoch_1600.png" alt="Messi Epoch 1600">
                </div>
                <div class="comparison-label">Epoch 1600</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/epoch_2999.png" alt="Messi Epoch 2999">
                </div>
                <div class="comparison-label">Final (Epoch 2999)</div>
            </div>
        </div>

        <h3>Hyperparameter Analysis</h3>
        <p>I experimented with different positional encoding frequencies (L) and network widths:</p>

        <div class="quad-grid">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L2_W64.png" alt="L2 W64">
                </div>
                <div class="comparison-label">L=2, Width=64</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L2_W256.png" alt="L2 W256">
                </div>
                <div class="comparison-label">L=2, Width=256</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L10_W64.png" alt="L10 W64">
                </div>
                <div class="comparison-label">L=10, Width=64</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L10_W256.png" alt="L10 W256">
                </div>
                <div class="comparison-label">L=10, Width=256</div>
            </div>
        </div>

        <p>
            As expected, higher positional encoding frequencies (L=10) capture fine details better than lower frequencies (L=2). 
            Similarly, wider networks (256 units) produce smoother reconstructions compared to narrower ones (64 units). 
            The combination of high frequency encoding and sufficient network capacity is crucial for high-quality reconstruction.
        </p>

        <h3>Training Metrics</h3>
        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/psnr_curve.png" alt="PSNR Curve">
                </div>
                <div class="comparison-label">PSNR Training Curve</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/loss_curve.png" alt="Loss Curve">
                </div>
                <div class="comparison-label">Loss Training Curve</div>
            </div>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #4080ff;">
            <h2 id="part2" style="margin: 0; color: #4080ff;">Part 2: Neural Radiance Field from Multi-view Images</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">Full 3D NeRF implementation with volume rendering</div>
        </div>

        <p>
            Building on the 2D neural field foundation, I implemented a full Neural Radiance Field that represents 
            3D scenes from multi-view images. This involved creating rays from camera parameters, sampling points 
            along rays, implementing the NeRF network architecture, and performing volume rendering to synthesize 
            novel views.
        </p>

        <h3>Implementation Overview</h3>
        <div class="algorithm-box">
            <h4>Key Components:</h4>
            <ul>
                <li><strong>Ray Generation:</strong> Convert pixel coordinates to 3D rays using camera parameters</li>
                <li><strong>Point Sampling:</strong> Sample points along rays with random perturbation</li>
                <li><strong>NeRF Network:</strong> MLP that predicts density and color from 3D position and view direction</li>
                <li><strong>Volume Rendering:</strong> Composite samples along rays to produce final pixel colors</li>
                <li><strong>Training:</strong> Optimize network to match observed multi-view images</li>
            </ul>
        </div>

        <h3>Ray and Sample Visualizations</h3>
        <p>Visualization of camera frustums, sampled rays, and 3D sample points:</p>

        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container frustum-container ray-frustum-container">
                    <img src="proj4/part2_content/visualizations_rays/lego_render1.png" alt="Ray Visualization 1">
                </div>
                <div class="comparison-label">Camera Frustums and Ray Samples</div>
            </div>
            <div class="comparison-item">
                <div class="image-container frustum-container ray-frustum-container">
                    <img src="proj4/part2_content/visualizations_rays/lego_render2.png" alt="Ray Visualization 2">
                </div>
                <div class="comparison-label">3D Sample Points Visualization</div>
            </div>
        </div>

        <h3>Training Progression</h3>
        <p>Training progression showing the NeRF learning to represent the Lego scene:</p>

        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_200_epoch_render.png" alt="200 Epochs">
                </div>
                <div class="comparison-label">200 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_400_epoch_render.png" alt="400 Epochs">
                </div>
                <div class="comparison-label">400 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_800_epoch_render.png" alt="800 Epochs">
                </div>
                <div class="comparison-label">800 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_1500_epoch_render.png" alt="1500 Epochs">
                </div>
                <div class="comparison-label">1500 Epochs</div>
            </div>
        </div>

        <div class="large-comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_ground_truth.png" alt="Ground Truth">
                </div>
                <div class="comparison-label">Ground Truth</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_final_render.png" alt="Final">
                </div>
                <div class="comparison-label">Final Result</div>
            </div>
        </div>

        <h3>Training Metrics</h3>
        <div style="width: 60%; margin: 0 auto; text-align: center;">
            <div class="image-container">
                <img src="proj4/part2_content/lego_loss_psnr.png" alt="Lego Training Curve">
            </div>
            <div class="comparison-label">Training Loss and PSNR Curve</div>
        </div>

        <div class="psnr-info">
            <h4>Performance Results:</h4>
            <p>
                The final model achieved <strong>25.2 PSNR</strong> on the validation set after 1000 training iterations 
                with a batch size of 10k rays. Training used Adam optimizer with learning rate 5e-4. The model successfully 
                learns to represent the complex geometry and appearance of the Lego scene.
            </p>
        </div>

        <h3>Novel View Synthesis</h3>
        <p>Spherical rendering showing novel views of the Lego scene:</p>

        <div class="video-container">
            <video width="600" loop autoplay muted>
                <source src="proj4/part2_content/lego_nerf_video_circular_orbit.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div style="text-align: center; color: #808080; margin-top: 10px;">
            Lego NeRF - Spherical Novel View Rendering
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #ff8040;">
            <h2 id="part2_6" style="margin: 0; color: #ff8040;">Part 2.6: Training NeRF on Own Data</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">NeRF trained on personally captured object</div>
        </div>

        <p>
            Using the dataset I created in Part 0, I trained a NeRF on my own captured object. This involved 
            adapting the network hyperparameters for real-world data, adjusting the near/far sampling bounds, 
            and fine-tuning the training process to handle the challenges of real camera data compared to the 
            synthetic Lego scene.
        </p>

        <h3>Hyperparameter Adjustments</h3>
        <div class="algorithm-box">
            <h4>Key Changes for Real Data:</h4>
            <ul>
                <li><strong>Near/Far Bounds:</strong> Adjusted to near=0.02, far=0.5 (vs 2.0-6.0 for Lego)</li>
                <li><strong>Samples per Ray:</strong> Increased to 64 samples for better quality</li>
                <li><strong>Learning Rate:</strong> Reduced to 1e-4 for more stable training</li>
                <li><strong>Image Resolution:</strong> Resized images to manageable resolution for training</li>
                <li><strong>Training Iterations:</strong> Extended to 8000+ iterations for convergence</li>
            </ul>
        </div>

        <h3>Training Progression</h3>
        <p>Training progression showing the NeRF learning to represent my captured object:</p>

        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_2000_epochs.png" alt="2000 Epochs">
                </div>
                <div class="comparison-label">2000 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_4000_epochs.png" alt="4000 Epochs">
                </div>
                <div class="comparison-label">4000 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_6000_epochs.png" alt="6000 Epochs">
                </div>
                <div class="comparison-label">6000 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_8000_epochs.png" alt="8000 Epochs">
                </div>
                <div class="comparison-label">8000 Epochs</div>
            </div>
        </div>

        <div class="large-comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_ground_truth.png" alt="Ground Truth">
                </div>
                <div class="comparison-label">Ground Truth</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_final_epochs.png" alt="Final">
                </div>
                <div class="comparison-label">Final Result</div>
            </div>
        </div>

        <h3>Training Metrics</h3>
        <div style="width: 60%; margin: 0 auto; text-align: center;">
            <div class="image-container">
                <img src="proj4/part2.6_content/vaja_psnr_loss.png" alt="Own Data Training Curve">
            </div>
            <div class="comparison-label">Training Loss and PSNR Curve for Own Data</div>
        </div>

        <h3>Novel View Synthesis</h3>
        <p>Circular orbit rendering of my captured object:</p>

        <div class="video-container">
            <video width="600" loop autoplay muted>
                <source src="proj4/part2.6_content/vaja_nerf_video_circular_orbit.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div style="text-align: center; color: #808080; margin-top: 10px;">
            Own Data NeRF - Circular Novel View Rendering
        </div>

        <h3>Challenges and Solutions</h3>
        <div class="highlight">
            <h4>Real Data Challenges:</h4>
            <ul>
                <li><strong>Lighting Variations:</strong> Real capture had slight lighting changes between views</li>
                <li><strong>Motion Blur:</strong> Some images had minor blur requiring careful filtering</li>
                <li><strong>Calibration Precision:</strong> ArUco detection required fine-tuning for consistent results</li>
                <li><strong>Convergence Time:</strong> Real data required longer training compared to synthetic scenes</li>
            </ul>
            <h4>Solutions Applied:</h4>
            <ul>
                <li>Increased sampling density along rays for better detail capture</li>
                <li>Adjusted near/far bounds based on actual scene depth</li>
                <li>Used more conservative learning rates for stable training</li>
                <li>Extended training iterations for proper convergence</li>
            </ul>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 30px; margin: 60px -20px 40px; border-left: 3px solid #ff8040;">
            <h2 id="summary" style="margin: 0; color: #ff8040;">Project Summary</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">Complete Neural Radiance Field implementation and results</div>
        </div>

        <p>
            This project successfully implemented a complete Neural Radiance Field pipeline, from basic 2D neural 
            fields to full 3D scene representation with novel view synthesis. The implementation demonstrates the 
            power of neural implicit representations for computer vision and graphics applications.
        </p>

        <h3>Key Achievements</h3>
        <ul>
            <li><strong>Camera Calibration:</strong> Implemented ArUco-based calibration and pose estimation</li>
            <li><strong>2D Neural Fields:</strong> Successfully fitted MLPs to represent 2D images with positional encoding</li>
            <li><strong>3D NeRF Implementation:</strong> Built complete volume rendering pipeline with ray sampling</li>
            <li><strong>Novel View Synthesis:</strong> Generated high-quality novel views of both synthetic and real scenes</li>
            <li><strong>Real Data Training:</strong> Adapted NeRF to work with personally captured multi-view data</li>
        </ul>

        <h3>Technical Insights</h3>
        <ul>
            <li><strong>Positional Encoding:</strong> High-frequency encodings are crucial for capturing fine details</li>
            <li><strong>Volume Rendering:</strong> Proper ray sampling and alpha compositing enable photorealistic synthesis</li>
            <li><strong>Network Architecture:</strong> Skip connections and view-dependent coloring improve reconstruction quality</li>
            <li><strong>Real vs Synthetic:</strong> Real data requires more careful preprocessing and longer training</li>
        </ul>

        <h3>Performance Results</h3>
        <div class="psnr-info">
            <h4>Final PSNR Scores:</h4>
            <ul>
                <li><strong>Lego Scene (Synthetic):</strong> 25.2 PSNR after 1000 iterations</li>
                <li><strong>Own Data (Real):</strong> Successful novel view synthesis with good visual quality</li>
                <li><strong>2D Neural Fields:</strong> High-quality image reconstruction with proper hyperparameters</li>
            </ul>
        </div>

        <p>
            The project demonstrates that neural radiance fields are a powerful tool for 3D scene representation, 
            capable of generating photorealistic novel views from sparse multi-view input. The techniques learned 
            here form the foundation for advanced 3D computer vision and graphics applications.
        </p>

        <div style="text-align: center; color: #606060; margin-top: 80px; padding: 40px 0; border-top: 1px solid #2a2a2a;">
            © 2025 Sukhamrit Singh. All rights reserved.
        </div>
    </div>
</body>
</html>