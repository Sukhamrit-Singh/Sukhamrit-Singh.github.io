<!DOCTYPE html>
<html>
<head>
    <title>Project 4 - CS180</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            line-height: 1.8;
            color: #e0e0e0;
            background: #0a0a0a;
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 60px 80px;
        }

        h1 {
            font-size: 2.5em;
            font-weight: 300;
            letter-spacing: -1px;
            color: #ffffff;
            margin-bottom: 8px;
        }

        .subtitle {
            font-size: 1em;
            color: #808080;
            margin-bottom: 60px;
            font-weight: 400;
        }

        .author {
            color: #606060;
            margin: 10px 0;
            font-size: 1em;
        }

        h2 {
            color: #ffffff;
            font-size: 1.8em;
            font-weight: 300;
            margin: 80px 0 40px;
            letter-spacing: -0.5px;
        }

        h3 {
            color: #c0c0c0;
            font-size: 1.3em;
            font-weight: 300;
            margin: 40px 0 20px;
        }

        p {
            color: #a0a0a0;
            margin-bottom: 20px;
            line-height: 1.8;
        }

        ul, ol {
            color: #a0a0a0;
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 8px;
        }

        code {
            background: #1a1a1a;
            color: #80ff80;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: inherit;
        }

        pre {
            background: #1a1a1a;
            color: #80ff80;
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: inherit;
        }

        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .image-wrapper {
            margin-bottom: 20px;
        }

        .image-container {
            position: relative;
            overflow: hidden;
            background: #1a1a1a;
            border-radius: 4px;
            transition: transform 0.2s ease;
        }

        .image-container:hover {
            transform: scale(1.02);
        }

        .image-container img {
            width: 100%;
            height: auto;
            display: block;
            opacity: 0.9;
            transition: opacity 0.2s ease;
        }

        .image-container:hover img {
            opacity: 1;
        }

        .image-caption {
            color: #ffffff;
            font-size: 0.95em;
            margin-top: 10px;
            text-align: center;
        }

        .offset-info {
            font-size: 0.85em;
            color: #606060;
            margin-top: 4px;
            text-align: center;
        }

        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 40px 0;
        }

        .comparison-item {
            text-align: center;
        }

        .comparison-item img {
            width: 100%;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .comparison-label {
            color: #808080;
            font-size: 0.9em;
        }

        .algorithm-box {
            background: #1a1a1a;
            border-left: 3px solid #404040;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .highlight {
            background: #1a1a1a;
            border-left: 3px solid #ffff00;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin: 40px 0;
        }

        .results-table th {
            background: #1a1a1a;
            color: #ffffff;
            padding: 12px;
            text-align: left;
            font-weight: 400;
        }

        .results-table td {
            padding: 10px;
            border-bottom: 1px solid #2a2a2a;
            color: #a0a0a0;
        }

        .results-table tr:hover {
            background: #151515;
        }

        .back-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: #1a1a1a;
            border: 1px solid #404040;
            color: #808080;
            padding: 8px 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 8px;
            text-decoration: none;
            transition: all 0.2s ease;
            z-index: 1000;
            font-size: 14px;
        }
        
        .back-button:hover {
            background: #2a2a2a;
            color: #ffffff;
            border-color: #606060;
        }

        .triple-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 40px 0;
        }

        .quad-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            grid-template-rows: 1fr 1fr;
            gap: 20px;
            margin: 40px 0;
        }

        .training-progression {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }

        .video-container {
            display: flex;
            justify-content: center;
            margin: 40px 0;
        }

        .video-container video {
            max-width: 100%;
            border-radius: 4px;
        }

        .psnr-info {
            background: #1a1a1a;
            border-left: 3px solid #40ff40;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .large-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 40px 0;
            max-width: 80%;
            margin-left: auto;
            margin-right: auto;
        }

        .frustum-container {
            background: #f0f0f0;
            border-radius: 4px;
            padding: 20px;
            aspect-ratio: 1920 / 934;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .frustum-container img {
            width: 100%;
            height: 100%;
            object-fit: contain;
        }

        .ray-frustum-container img {
            width: 120%;
            height: 120%;
            object-fit: contain;
            transform: translateY(8%);
        }

        @media (max-width: 768px) {
            .container {
                padding: 40px;
            }
            
            .image-grid {
                grid-template-columns: 1fr;
            }
            
            .comparison {
                grid-template-columns: 1fr;
            }

            .triple-comparison {
                grid-template-columns: 1fr;
            }

            .quad-grid {
                grid-template-columns: 1fr;
                grid-template-rows: auto;
            }
        }

        .equation {
            background: transparent;
            color: #80ff80;
            padding: 12px 6px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 6px 0;
            font-family: inherit;
            white-space: nowrap;
        }

        .matrix-block {
            display: flex;
            gap: 20px;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            margin: 18px 0;
        }

        .architecture-diagram {
            background: #1a1a1a;
            border-radius: 8px;
            padding: 30px;
            margin: 30px 0;
            overflow-x: auto;
        }

        .layer {
            display: inline-block;
            background: #2a2a2a;
            border: 2px solid #404040;
            border-radius: 6px;
            padding: 15px 10px;
            margin: 5px;
            text-align: center;
            min-width: 80px;
            position: relative;
            vertical-align: middle;
        }

        .layer.input {
            background: #1a4a1a;
            border-color: #40ff40;
            color: #40ff40;
        }

        .layer.hidden {
            background: #1a1a4a;
            border-color: #4080ff;
            color: #4080ff;
        }

        .layer.output {
            background: #4a1a1a;
            border-color: #ff4040;
            color: #ff4040;
        }

        .layer.pe {
            background: #4a4a1a;
            border-color: #ffff40;
            color: #ffff40;
        }

        .layer.skip {
            background: #4a1a4a;
            border-color: #ff40ff;
            color: #ff40ff;
        }

        .arrow {
            display: inline-block;
            margin: 0 10px;
            color: #808080;
            font-size: 20px;
            vertical-align: middle;
        }

        .layer-label {
            font-size: 0.85em;
            font-weight: bold;
        }

        .layer-dim {
            font-size: 0.7em;
            color: #c0c0c0;
            margin-top: 3px;
        }

        .activation {
            font-size: 0.6em;
            color: #a0a0a0;
            margin-top: 2px;
            font-style: italic;
        }

        .architecture-flow {
            display: flex;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
        }

        .branch {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
        }

        .concat {
            background: #2a4a2a;
            border: 2px solid #60ff60;
            color: #60ff60;
            border-radius: 6px;
            padding: 8px 12px;
            font-size: 0.8em;
        }

        .nerf-architecture {
            display: flex;
            flex-direction: column;
            gap: 20px;
            background: #1a1a1a;
            padding: 30px;
            border-radius: 8px;
            margin: 30px 0;
        }

        .nerf-flow {
            display: flex;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 15px 0;
        }

        .nerf-input-section {
            border: 1px dashed #404040;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }

        .skip-connection {
            position: relative;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .skip-arrow {
            position: absolute;
            top: -30px;
            left: 50%;
            transform: translateX(-50%);
            color: #ff40ff;
            font-size: 12px;
            white-space: nowrap;
        }

        /* Ensure MathJax renders in regular text color */
        .MathJax, .MathJax * {
            color: #ffffff !important;
        }
        
        /* For inline math */
        .MathJax_Display {
            color: #ffffff !important;
        }

        /* For any green math text */
        mjx-math, mjx-math * {
            color: #ffffff !important;
        }
    </style>
    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>
<body>
    <a href="./index.html" class="back-button" title="Back to portfolio">
        <span style="font-size: 14px;">../</span>
    </a>
    
    <div class="container">
        <div style="margin-bottom: 60px;">
            <h1 style="margin-bottom: 20px;">proj4/</h1>
            <div style="margin-left: 40px;">
                <div style="margin: 8px 0;">
                    <a href="#overview" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">overview/</span>
                        <span style="color: #606060; margin-left: 8px;">Neural Radiance Fields</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part0" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part0/</span>
                        <span style="color: #606060; margin-left: 8px;">Camera Calibration and 3D Scanning</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part1" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part1/</span>
                        <span style="color: #606060; margin-left: 8px;">2D Neural Field</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part2" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part2/</span>
                        <span style="color: #606060; margin-left: 8px;">3D Neural Radiance Field</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#part2_6" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">├── </span>
                        <span style="color: #ffffff;">part2.6/</span>
                        <span style="color: #606060; margin-left: 8px;">Own Data NeRF</span>
                    </a>
                </div>
                <div style="margin: 8px 0;">
                    <a href="#summary" style="color: #808080; text-decoration: none; transition: color 0.2s ease;">
                        <span style="color: #404040;">└── </span>
                        <span style="color: #ffffff;">summary/</span>
                        <span style="color: #606060; margin-left: 8px;">Project Summary and Results</span>
                    </a>
                </div>
            </div>
        </div>

        <h1 style="font-size: 2.5em; margin-bottom: 10px;">Neural Radiance Fields!</h1>
        <div style="color: #808080; margin-bottom: 40px; font-size: 1em;">CS180 Project 4: Neural Radiance Fields and Volume Rendering</div>

        <h2 id="overview">Overview</h2>
        <p>
            This project explores Neural Radiance Fields (NeRF), a cutting-edge technique for 3D scene representation 
            and novel view synthesis. We implement neural networks that learn implicit representations of 3D scenes 
            from multi-view images, enabling photorealistic novel view synthesis. The project spans from 2D neural 
            fields to full 3D NeRF implementation, including camera calibration, volume rendering, and training on 
            both synthetic and real-world data.
        </p>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #ff4040;">
            <h2 id="part0" style="margin: 0; color: #ff4040;">Part 0: Camera Calibration and 3D Scanning</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">ArUco-based camera calibration and pose estimation</div>
        </div>

        <p>
            For this section, I calibrated my camera using ArUco tags and captured multi-view images of a chosen object. 
            The calibration process involved capturing 30-50 images of ArUco calibration tags, extracting corner coordinates, 
            and using OpenCV's camera calibration to compute intrinsic parameters and distortion coefficients.
        </p>


        <p>
            After calibration, I captured images of my chosen object (a small figurine) alongside a single ArUco tag, 
            then used the calibrated camera parameters to estimate camera poses via solvePnP. The final step involved 
            undistorting images and packaging everything into a dataset format compatible with NeRF training.
        </p>

        <h3>Sample Images from Data Capture</h3>
        <p>Examples of captured images showing the object alongside the ArUco tag for pose estimation:</p>

        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part0_content/IMG_2311 Large.jpeg" alt="Sample Image 1">
                </div>
                <div class="comparison-label">Captured Image 1</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part0_content/IMG_2324 Large.jpeg" alt="Sample Image 2">
                </div>
                <div class="comparison-label">Captured Image 2</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part0_content/IMG_2346 Large.jpeg" alt="Sample Image 3">
                </div>
                <div class="comparison-label">Captured Image 3</div>
            </div>
        </div>

        <h3>Camera Frustum Visualizations</h3>
        <p>Screenshots of the camera frustums visualization in Viser, showing the estimated camera poses:</p>

        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container frustum-container">
                    <img src="proj4/part0_content/vaja_frustum1.png" alt="Camera Frustums 1">
                </div>
                <div class="comparison-label">Camera Frustums Visualization - View 1</div>
            </div>
            <div class="comparison-item">
                <div class="image-container frustum-container">
                    <img src="proj4/part0_content/vaja_frustum2.png" alt="Camera Frustums 2">
                </div>
                <div class="comparison-label">Camera Frustums Visualization - View 2</div>
            </div>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #40ff40;">
            <h2 id="part1" style="margin: 0; color: #40ff40;">Part 1: 2D Neural Field</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">Fitting neural networks to represent 2D images</div>
        </div>

        <p>
            Before implementing 3D NeRF, I started with a simpler 2D version to understand the fundamentals. 
            I created a Multilayer Perceptron (MLP) with Sinusoidal Positional Encoding that takes 2D pixel 
            coordinates as input and outputs RGB color values. The network was trained to fit entire images 
            by optimizing pixel-wise color predictions.
        </p>

        <h3>2D Neural Field Architecture</h3>
        
        <div style="text-align: center; margin: 30px auto; max-width: 75%;">
            <img src="proj4/2d_nn_design.jpg" alt="2D Neural Field Architecture" style="width: 100%; height: auto; border-radius: 4px;">
        </div>

        <div class="algorithm-box">
            <h4>Network Architecture Details:</h4>
            <ul>
                <li><strong>Number of Layers:</strong> 4-layer MLP (3 hidden layers + output layer)</li>
                <li><strong>Width:</strong> 256 hidden units per layer</li>
                <li><strong>Input Dimension:</strong> $L \times 2 \times 2 + 2 = 42$ for $L=10$ positional encoding</li>
                <li><strong>Output Dimension:</strong> 3 (RGB color values)</li>
                <li><strong>Activation Functions:</strong> ReLU for hidden layers, Sigmoid for output</li>
                <li><strong>Positional Encoding:</strong> Sinusoidal PE with $L=10$ frequency levels</li>
                <li><strong>Learning Rate:</strong> $1 \times 10^{-2}$ with Adam optimizer</li>
                <li><strong>Batch Size:</strong> 10,000 pixels sampled per epoch</li>
            </ul>
        </div>

        <h3>Training Progression</h3>
        <p>Below shows the training progression for both the provided fox image and my own Messi image:</p>

        <h4>Fox Image Training</h4>
        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/reconstruction_epoch_0.png" alt="Fox Epoch 0">
                </div>
                <div class="comparison-label">Epoch 0</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/reconstruction_epoch_100.png" alt="Fox Epoch 100">
                </div>
                <div class="comparison-label">Epoch 100</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/reconstruction_epoch_400.png" alt="Fox Epoch 400">
                </div>
                <div class="comparison-label">Epoch 400</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/reconstruction_epoch_1600.png" alt="Fox Epoch 1600">
                </div>
                <div class="comparison-label">Epoch 1600</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/reconstruction_epoch_2999.png" alt="Fox Epoch 2999">
                </div>
                <div class="comparison-label">Final (Epoch 2999)</div>
            </div>
        </div>

        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/part1_image.jpg" alt="Original Fox Image">
                </div>
                <div class="comparison-label">Original Fox Image</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/fox/reconstruction_epoch_2999.png" alt="Fox Final Reconstruction">
                </div>
                <div class="comparison-label">Neural Field Reconstruction</div>
            </div>
        </div>

        <h4>Messi Image Training</h4>
        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/reconstruction_epoch_0.png" alt="Messi Epoch 0">
                </div>
                <div class="comparison-label">Epoch 0</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/reconstruction_epoch_100.png" alt="Messi Epoch 100">
                </div>
                <div class="comparison-label">Epoch 100</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/reconstruction_epoch_400.png" alt="Messi Epoch 400">
                </div>
                <div class="comparison-label">Epoch 400</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/reconstruction_epoch_1600.png" alt="Messi Epoch 1600">
                </div>
                <div class="comparison-label">Epoch 1600</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/reconstruction_epoch_2999.png" alt="Messi Epoch 2999">
                </div>
                <div class="comparison-label">Final (Epoch 2999)</div>
            </div>
        </div>

        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/messi.jpg" alt="Original Messi Image">
                </div>
                <div class="comparison-label">Original Messi Image</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/training_progressions/messi/reconstruction_epoch_2999.png" alt="Messi Final Reconstruction">
                </div>
                <div class="comparison-label">Neural Field Reconstruction</div>
            </div>
        </div>

        <h3>Hyperparameter Analysis</h3>
        <p>I experimented with different positional encoding frequencies (L) and network widths:</p>

        <div class="quad-grid">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L2_W64.png" alt="L2 W64">
                </div>
                <div class="comparison-label">$L=2$, Width=64</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L2_W256.png" alt="L2 W256">
                </div>
                <div class="comparison-label">$L=2$, Width=256</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L10_W64.png" alt="L10 W64">
                </div>
                <div class="comparison-label">$L=10$, Width=64</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/final_results/reconstruction_L10_W256.png" alt="L10 W256">
                </div>
                <div class="comparison-label">$L=10$, Width=256</div>
            </div>
        </div>

        <p>
            As expected, higher positional encoding frequencies ($L=10$) capture fine details better than lower frequencies ($L=2$). 
            Similarly, wider networks (256 units) produce smoother reconstructions compared to narrower ones (64 units).
        </p>

        <h3>Training Metrics</h3>
        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/psnr_curve.png" alt="PSNR Curve">
                </div>
                <div class="comparison-label">PSNR Training Curve (Fox Image)</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part1_content/loss_curve.png" alt="Loss Curve">
                </div>
                <div class="comparison-label">Loss Training Curve (Fox Image)</div>
            </div>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #4080ff;">
            <h2 id="part2" style="margin: 0; color: #4080ff;">Part 2: Neural Radiance Field from Multi-view Images</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">Full 3D NeRF implementation with volume rendering</div>
        </div>

        <p>
            Building on the 2D neural field foundation, I implemented a full Neural Radiance Field that represents 
            3D scenes from multi-view images using inverse rendering from calibrated cameras.
        </p>

        <h3>Part 2.1: Create Rays from Cameras</h3>
        <p>
            I implemented three core coordinate transformations to generate camera rays. First, I used 4×4 transformation matrices to convert points from camera space to world coordinates: $\mathbf{x}_w = \mathbf{R} \mathbf{x}_c + \mathbf{t}$. Then I inverted the camera intrinsic matrix to convert pixel coordinates to 3D camera coordinates: $\mathbf{x}_c = K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$. Finally, I generated rays by computing the origin as the camera position $\mathbf{o} = \mathbf{t}$ and the direction as the normalized vector from camera center through each pixel.
        </p>

        <h3>Part 2.2: Sampling</h3>
        <p>
            I implemented random ray sampling from multi-view images, adding 0.5 to UV coordinates to sample from pixel centers rather than corners. For each ray, I uniformly sampled 64 points between near and far planes by dividing the ray into equal intervals. During training, I added random perturbations $t_i = t_i + \mathcal{U}(0, \Delta t)$ to sample positions to prevent overfitting and ensure the model explores all locations along each ray.
        </p>

        <h3>Part 2.3: Putting the Dataloading All Together</h3>
        <p>
            I created a unified $\texttt{RaysData}$ class that precomputes ray origins and directions for all training images, then efficiently samples 10,000 random rays per iteration. This dataloader returns ray origins, directions, and corresponding ground truth RGB colors for batch training. I verified the implementation by visualizing camera frustums, rays, and 3D sample points using Viser.
        </p>

        <div class="comparison">
            <div class="comparison-item">
                <div class="image-container frustum-container">
                    <img src="proj4/part2_content/visualizations_rays/lego_render1.png" alt="Rays Visualization 1">
                </div>
                <div class="comparison-label">Camera Frustums and Rays - View 1</div>
            </div>
            <div class="comparison-item">
                <div class="image-container frustum-container">
                    <img src="proj4/part2_content/visualizations_rays/lego_render2.png" alt="Rays Visualization 2">
                </div>
                <div class="comparison-label">Camera Frustums and Rays - View 2</div>
            </div>
        </div>

        <h3>Part 2.4: Neural Radiance Field</h3>
        <p>
            I built an 8-layer MLP that takes positionally-encoded 3D coordinates ($L=10$) and view directions ($L=4$) as input. The network uses skip connections at layer 5 and has separate heads for density prediction (with ReLU) and view-dependent color prediction (with Sigmoid). This architecture allows the model to capture both geometric structure and view-dependent appearance effects like specular highlights.
        </p>

        <h3>3D Neural Radiance Field Architecture</h3>
        
        <div style="text-align: center; margin: 30px auto; max-width: 75%;">
            <img src="proj4/3d_nn_design.png" alt="3D Neural Radiance Field Architecture" style="width: 100%; height: auto; border-radius: 4px;">
        </div>

        <h3>Part 2.5: Volume Rendering</h3>
        <p>
            I implemented the discrete volume rendering equation to composite colors and densities along rays into final pixel colors: $C(\mathbf{r}) = \sum_{i=1}^{N} T_i \alpha_i \mathbf{c}_i$. The function computes alpha values $\alpha_i = 1 - \exp(-\sigma_i \delta_i)$ and transmittance $T_i = \exp(-\sum_{j=1}^{i-1} \sigma_j \delta_j)$ using cumulative sums for proper alpha compositing. My implementation passed the provided torch.allclose() test, ensuring correct mathematical computation of the rendering integral.
        </p>

        <h3>Training Progression</h3>
        <p>Training progression showing the NeRF learning to represent the Lego scene:</p>

        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_200_epoch_render.png" alt="200 Epochs">
                </div>
                <div class="comparison-label">200 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_400_epoch_render.png" alt="400 Epochs">
                </div>
                <div class="comparison-label">400 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_800_epoch_render.png" alt="800 Epochs">
                </div>
                <div class="comparison-label">800 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_1500_epoch_render.png" alt="1500 Epochs">
                </div>
                <div class="comparison-label">1500 Epochs</div>
            </div>
        </div>

        <div class="large-comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_ground_truth.png" alt="Ground Truth">
                </div>
                <div class="comparison-label">Ground Truth</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2_content/training_progression/lego_final_render.png" alt="Final">
                </div>
                <div class="comparison-label">Final Result</div>
            </div>
        </div>

        <h3>Training Metrics</h3>
        <div style="width: 60%; margin: 0 auto; text-align: center;">
            <div class="image-container">
                <img src="proj4/part2_content/lego_loss_psnr.png" alt="Lego Training Curve">
            </div>
            <div class="comparison-label">Training Loss and PSNR Curve</div>
        </div>

        <div class="psnr-info">
            <h4>Performance Results:</h4>
            <p>
                The final model achieved <strong>25.2 dB PSNR</strong> on the validation set after 1000 training iterations 
                with a batch size of 10k rays. Training used Adam optimizer with learning rate $5 \times 10^{-4}$. The model successfully 
                learns to represent the complex geometry and appearance of the Lego scene.
            </p>
        </div>

        <h3>Novel View Synthesis</h3>
        <p>Spherical rendering showing novel views of the Lego scene:</p>

        <div class="video-container">
            <video width="600" loop autoplay muted>
                <source src="proj4/part2_content/lego_nerf_video_circular_orbit.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div style="text-align: center; color: #808080; margin-top: 10px;">
            Lego NeRF - Spherical Novel View Rendering
        </div>

        <h3>Volume Rendering Implementation</h3>
        <div class="algorithm-box">
            <h4>Discrete Volume Rendering Equation:</h4>
            <div class="equation">
                $$C(\mathbf{r}) = \sum_{i=1}^{N} T_i \alpha_i \mathbf{c}_i$$
                $$\text{where } T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right), \quad \alpha_i = 1 - \exp(-\sigma_i \delta_i)$$
            </div>
            <ul>
                <li><strong>Alpha Values:</strong> $\alpha_i = 1 - \exp(-\sigma_i \cdot \delta_i)$ for absorption probability</li>
                <li><strong>Transmittance:</strong> $T_i$ computed using torch.cumsum() for ray termination probability</li>
                <li><strong>Weight Calculation:</strong> $w_i = T_i \cdot \alpha_i$ for proper alpha compositing</li>
                <li><strong>Final Color:</strong> Weighted sum $C(\mathbf{r}) = \sum_{i=1}^{N} w_i \mathbf{c}_i$</li>
                <li><strong>Vectorization:</strong> Entire batch of 10k rays processed simultaneously for efficiency</li>
                <li><strong>Step Size:</strong> $\delta_i = \frac{z_{\text{far}} - z_{\text{near}}}{N_{\text{samples}}}$ with 64 samples per ray</li>
                <li><strong>Validation:</strong> Passed torch.allclose() test with reference implementation</li>
            </ul>
        </div>

        <h3>Implementation Overview</h3>
        <div class="algorithm-box">
            <h4>Key Components Implemented:</h4>
            <ul>
                <li><strong>Camera-to-World Transform:</strong> Implemented `transform(c2w, x_c)` to convert camera coordinates to world space using 4x4 transformation matrices</li>
                <li><strong>Pixel-to-Camera Conversion:</strong> `pixel_to_camera(K, uv, s)` converts 2D pixel coordinates to 3D camera coordinates using camera intrinsics</li>
                <li><strong>Ray Generation:</strong> `pixel_to_ray(K, c2w, uv)` generates ray origins and normalized directions from pixel coordinates</li>
                <li><strong>Point Sampling:</strong> `sample_along_rays(rays_o, rays_d, n_samples, near, far, perturb)` samples 64 points with explicit near/far bounds ($z_{\text{near}}=2.0$, $z_{\text{far}}=6.0$ for Lego)</li>
                <li><strong>NeRF Network:</strong> 8-layer MLP with skip connections, predicting density (ReLU) and view-dependent RGB (sigmoid)</li>
                <li><strong>Volume Rendering:</strong> Discrete alpha compositing using cumulative products for ray termination probability</li>
            </ul>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 20px; margin: 60px -20px 40px; border-left: 3px solid #ff8040;">
            <h2 id="part2_6" style="margin: 0; color: #ff8040;">Part 2.6: Training NeRF on Own Data</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">NeRF trained on personally captured object</div>
        </div>

        <p>
            Using the dataset I created in Part 0, I trained a NeRF on my own captured object. This involved 
            adapting the network hyperparameters for real-world data, adjusting the near/far sampling bounds, 
            and fine-tuning the training process to handle the challenges of real camera data compared to the 
            synthetic Lego scene.
        </p>

        <h3>Hyperparameter Adjustments for Real Data</h3>
        <div class="algorithm-box">
            <h4>Critical Changes from Synthetic to Real Data:</h4>
            <ul>
                <li><strong>Near/Far Bounds:</strong> $z_{\text{near}}=0.02$, $z_{\text{far}}=0.5$ (vs $z_{\text{near}}=2.0$, $z_{\text{far}}=6.0$ for Lego) - adjusted for close-up object capture</li>
                <li><strong>Camera Intrinsics:</strong> Used actual calibrated K matrix instead of synthetic focal length</li>
                <li><strong>Samples per Ray:</strong> 64 samples (consistent) but denser due to smaller depth range</li>
                <li><strong>Learning Rate:</strong> $5 \times 10^{-4}$ (maintained from Lego) but longer training needed</li>
                <li><strong>Training Duration:</strong> 10,000 iterations (vs 2,000 for Lego) due to real-world complexity</li>
                <li><strong>Image Preprocessing:</strong> Undistortion with cv2.getOptimalNewCameraMatrix() and ROI cropping</li>
                <li><strong>Batch Size:</strong> 10k rays per iteration (maintained)</li>
                <li><strong>Random Seed:</strong> Fixed seed=42 for reproducible results</li>
            </ul>
        </div>

        <h3>Real Data Challenges Encountered</h3>
        <div class="highlight">
            <h4>Implementation Challenges:</h4>
            <ul>
                <li><strong>ArUco Detection:</strong> Required careful tag placement with white borders for consistent detection</li>
                <li><strong>Camera Calibration:</strong> 30-50 calibration images needed with varied angles/distances</li>
                <li><strong>Pose Estimation:</strong> solvePnP occasionally failed; implemented robust error handling</li>
                <li><strong>Image Distortion:</strong> Real phone camera required undistortion + principal point adjustment</li>
                <li><strong>Lighting Consistency:</strong> Captured images under uniform lighting to avoid brightness variations</li>
                <li><strong>Motion Blur:</strong> Required stable capture technique to avoid blurry training images</li>
            </ul>
        </div>

        <h3>Training Progression</h3>
        <p>Training progression showing the NeRF learning to represent my captured object:</p>

        <div class="training-progression">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_2000_epochs.png" alt="2000 Epochs">
                </div>
                <div class="comparison-label">2000 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_4000_epochs.png" alt="4000 Epochs">
                </div>
                <div class="comparison-label">4000 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_6000_epochs.png" alt="6000 Epochs">
                </div>
                <div class="comparison-label">6000 Epochs</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_8000_epochs.png" alt="8000 Epochs">
                </div>
                <div class="comparison-label">8000 Epochs</div>
            </div>
        </div>

        <div class="large-comparison">
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_ground_truth.png" alt="Ground Truth">
                </div>
                <div class="comparison-label">Ground Truth</div>
            </div>
            <div class="comparison-item">
                <div class="image-container">
                    <img src="proj4/part2.6_content/intermediate/vaja_final_epochs.png" alt="Final">
                </div>
                <div class="comparison-label">Final Result</div>
            </div>
        </div>

        <h3>Training Metrics</h3>
        <div style="width: 60%; margin: 0 auto; text-align: center;">
            <div class="image-container">
                <img src="proj4/part2.6_content/vaja_psnr_loss.png" alt="Own Data Training Curve">
            </div>
            <div class="comparison-label">Training Loss and PSNR Curve for Own Data</div>
        </div>

        <h3>Novel View Synthesis</h3>
        <p>Circular orbit rendering of my captured object:</p>

        <div class="video-container">
            <video width="600" loop autoplay muted>
                <source src="proj4/part2.6_content/vaja_nerf_video_circular_orbit.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div style="text-align: center; color: #808080; margin-top: 10px;">
            Own Data NeRF - Circular Novel View Rendering
        </div>

        <h3>Challenges and Solutions</h3>
        <div class="highlight">
            <h4>Real Data Challenges:</h4>
            <ul>
                <li><strong>Lighting Variations:</strong> Real capture had slight lighting changes between views</li>
                <li><strong>Motion Blur:</strong> Some images had minor blur requiring careful filtering</li>
                <li><strong>Calibration Precision:</strong> ArUco detection required fine-tuning for consistent results</li>
                <li><strong>Convergence Time:</strong> Real data required longer training compared to synthetic scenes</li>
            </ul>
            <h4>Solutions Applied:</h4>
            <ul>
                <li>Increased sampling density along rays for better detail capture</li>
                <li>Adjusted near/far bounds based on actual scene depth</li>
                <li>Used more conservative learning rates for stable training</li>
                <li>Extended training iterations for proper convergence</li>
            </ul>
        </div>

        <div style="background: linear-gradient(90deg, #1a1a1a 0%, transparent 100%); padding: 30px; margin: 60px -20px 40px; border-left: 3px solid #ff8040;">
            <h2 id="summary" style="margin: 0; color: #ff8040;">Project Summary</h2>
            <div style="color: #808080; font-size: 0.9em; margin-top: 5px;">Complete Neural Radiance Field implementation and results</div>
        </div>

        <p>
            This project successfully implemented a complete Neural Radiance Field pipeline, from basic 2D neural 
            fields to full 3D scene representation with novel view synthesis. The implementation demonstrates the 
            power of neural implicit representations for computer vision and graphics applications.
        </p>

        <h3>Key Achievements</h3>
        <ul>
            <li><strong>Camera Calibration:</strong> Implemented ArUco-based calibration and pose estimation</li>
            <li><strong>2D Neural Fields:</strong> Successfully fitted MLPs to represent 2D images with positional encoding</li>
            <li><strong>3D NeRF Implementation:</strong> Built complete volume rendering pipeline with ray sampling</li>
            <li><strong>Novel View Synthesis:</strong> Generated high-quality novel views of both synthetic and real scenes</li>
            <li><strong>Real Data Training:</strong> Adapted NeRF to work with personally captured multi-view data</li>
        </ul>

        <h3>Technical Insights</h3>
        <ul>
            <li><strong>Positional Encoding:</strong> High-frequency encodings are crucial for capturing fine details</li>
            <li><strong>Volume Rendering:</strong> Proper ray sampling and alpha compositing enable photorealistic synthesis</li>
            <li><strong>Network Architecture:</strong> Skip connections and view-dependent coloring improve reconstruction quality</li>
            <li><strong>Real vs Synthetic:</strong> Real data requires more careful preprocessing and longer training</li>
        </ul>

        <h3>Performance Results</h3>
        <div class="psnr-info">
            <h4>Quantitative Results Summary:</h4>
            <ul>
                <li><strong>Lego Scene (Synthetic):</strong> 25.2 dB PSNR after 2000 epochs, exceeded 23 dB PSNR target</li>
                <li><strong>Own Data NeRF:</strong> Successful 10,000 epoch training with convergent loss curve</li>
                <li><strong>2D Neural Fields (Messi):</strong> $\sim$24 dB PSNR final result with $L=10$, $W=256$</li>
                <li><strong>Volume Rendering Verification:</strong> Passed torch.allclose() assertion test</li>
                <li><strong>Camera Calibration:</strong> Successful pose estimation for 30+ images</li>
                <li><strong>Novel View Videos:</strong> Generated 120-frame circular orbit videos at 30 FPS</li>
            </ul>
        </div>

        <h3>Implementation Validation</h3>
        <div class="algorithm-box">
            <h4>Technical Verification:</h4>
            <ul>
                <li><strong>Volume Rendering Test:</strong> Implementation passed provided torch test with correct output values</li>
                <li><strong>Ray Generation:</strong> Verified UV coordinate mapping with assertion: images_train[0, sample_uvs[:,1], sample_uvs[:,0]] == dataset.pixels</li>
                <li><strong>Camera Transform:</strong> Verified inverse transform: x == transform(c2w.inv(), transform(c2w, x))</li>
                <li><strong>Data Pipeline:</strong> Successfully processed 30+ images with ArUco pose estimation</li>
                <li><strong>Training Convergence:</strong> Both synthetic and real data showed decreasing loss and increasing PSNR</li>
            </ul>
        </div>

        <p>
            The project demonstrates that neural radiance fields are a powerful tool for 3D scene representation, 
            capable of generating photorealistic novel views from sparse multi-view input. The techniques learned 
            here form the foundation for advanced 3D computer vision and graphics applications.
        </p>

        <div style="text-align: center; color: #606060; margin-top: 80px; padding: 40px 0; border-top: 1px solid #2a2a2a;">
            © 2025 Sukhamrit Singh. All rights reserved.
        </div>
    </div>
</body>
</html>